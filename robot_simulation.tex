%!TEX root = thesis.tex

\chapter{Robot simulation}
\label{chap:simulation}

This chapter focuses on the simulation part of the thesis. The objective is the creation of a simulation model, particularly designed for the IIS lab robot setup. This model needs to reflect the properties of the robot arms and the grippers as good as possible and provide physically similar behaviour. Certainly the simulated components have to provide the same control interface as their real counterparts, allowing to test and optimize control code on the simulator before utilizing it on the real robot. Preferably, the control code sees no difference about on which instance it is executed. The requirements to such a solution can be summarized as follows: 

\begin{itemize}

\item
The simulator needs to be able to generate realistic sensor and feedback data that can be used by the control software. This includes forces and torques that are measured within joints and tactile sensors, the current state of the various robot joints (position, velocity, effort) and also RGB and depth images, that are usually produced by Kinect cameras.

\item
The simulation solution has to provide exactly the same ROS control interface as the real robot. This interface essentially consists of a number of ROS topics, that can be used to send control commands to the various different robot components or to read actual joint states and sensor data.
 
\item
The utilized simulation platform has to provide a graphical user interface that allows to visualize the motions of the robot and its interaction with the environment. Possibly accidental collisions of  robot parts have to be registered and should also be visualized.

\item
In order to be used by as many people as possible, it is very important that the solution is really easy to use and does not require a long lead time. Therefore we put particular focus on usability and documentation.

\end{itemize}

The following sections explain the realization of this simulation solution step by step. The introduction section gives an overview about dynamic simulations and the requirements for the utilized simulation platform. After that the chosen simulation platform V-Rep is introduced and general information about the concepts and used terminology is given. Subsequent sections focus on the necessary steps to achieve the final solution, namely modelling required robot components, assembling and configuring the simulation scene and the implementation of the ROS control interface.

\section{Introduction}

Realistic simulation requires models that exactly describe the physical properties of the involved objects. A robot is formed from rigid elements called \emph{links}, connected by \emph{joints}. A link is a 3 dimensional body with a specific mass and moment of inertia. The texture of its surface depends on the utilized material. Joints are the flexible connections between the links. They are usually actuated by motors with a certain strength. Properties like the velocity of the robot's motions or the maximum possible payload it is able to bear highly depend on the strength of those joint motors. The sum of those physical parameters describes the dynamic properties of the robot. \\

The robot is used to perform motions and interact with objects within its environment. If the robot arm collides with an object it produces a collision reaction. The nature of this collision reaction depends on the physical parameters of the robot and the involved object. If the object is immovable, the robot motion will be stopped at a certain point. If the robot tries to grasp and lift a heavy object, it might slip through its fingers because the surface of the object may be to smooth and the gripper is not able to apply enough force to hold the object. \\

The required solution should be able to provide a realistic simulation of such dynamic interactions and reflect the physical behaviour of the robot as good as possible. Therefore the utilized simulation platform has to make use of a \emph{physics engine}. A physics engine is a software component, that is capable of computing parameters of physical processes based on the dynamic properties of the involved objects. Examples for such engines are the Open Dynamics Engine\footnote{http://www.ode.org} (ODE) and Bullet physics\footnote{http://bulletphysics.org}. We selected the simulation platform by taking the following criteria into account:

\begin{itemize}
\item
Which physics engine is used or is it possible to choose among various engines?
\item
Usability and stability
\item
Expandability
\item
Availability of required model components (arm model, gripper,\ldots)
\item
Quality of the documentation
\item
Licence issues
\end{itemize}

Candidates that have been taken into account were the Gazebo\footnote{http://gazebosim.org/} simulator and V-Rep\footnote{http://www.coppeliarobotics.com}. We finally decided to implement our project based on the V-Rep simulation platform because during initial tests it seemed to be much more stable than Gazebo and the user interface is very intuitive. Another important point was that V-Rep ships with a fully functional model of the Kuka LWR4+ robot arm.

\section{The Virtual Robot Experimentation Platform(V-Rep)}

This section gives an overview of the utilized simulation platform. V-Rep is a robot simulator, developed and maintained by Coppelia Robotics. The current version (V3.1.2) allows to choose among three configurable physics engines, namely ODE, Bullet and Vortex (only trial version) for simulating dynamic processes. The simulation environment is modelled, using the included scene editor. The distribution contains a large number of predefined models for different robot types. Additional modules provide functionalities related to collision visualization (\emph{collision detection module}) and inverse kinematics calculation (\emph{IK calculation module}). The behaviour of the simulator is highly customizable and its functionality can be extended via plugins. A plugin is a software library that is written in C++ and interacts with the simulator by using the provided programming API. V-Rep also allows to add embedded scripts to specific simulations. Those scripts need to be written in the scripting language LUA\footnote{http://www.lua.org}. V-Rep is no open source software but it provides a free licence for educational units and can therefore be used for research purposes. \cite{freese2013} gives a detailed insight into the V-Rep simulation platform.

%That editor can also be used to edit and simplify meshes. This is very important because for simulating dynamic processes only simple shapes with a low amount of vertices, edges and faces should be used to reduce complexity. The contained model browser provides a rich set of different robot models, static objects and various sensors, ready to use. A powerful feature are V-Rep's so called \emph{calculation modules}. They can be configured to provide additional calculation functionalities on groups of scene objects. The \emph{collision detection module} is capable of detecting and visualizing all kinds of collisions within the simulation scene. The \emph{inverse kinematics calculation module} allows to solve inverse kinematics problems for manipulators. The behaviour of the simulator is highly customizable via a rich programming API for C++ as well as the scripting language LUA\footnote{http://www.lua.org}. V-Rep is no open source software but it provides a free licence for educational units and can therefore be used for research purposes. \cite{freese2013} gives a more detailed insight into V-Rep's concepts .

\section{Dynamic simulations in V-REP}
\label{sec:vrep_intro}

For a better understanding of the modelling process it is necessary to explain a few fundamental concepts about designing simulations in V-Rep. This section covers the aspects that are important for the underlying project along with the used terminology. A detailed explanation can be found in the official V-Rep documentation\footnote{http://www.coppeliarobotics/helpFiles}. \\

Simulations in V-Rep are organized into \emph{scenes}. A scene represents the simulated world which is composed from a number of elements like three dimensional bodies, joints, sensors or cameras that are combined in a tree structure and arranged within the environment. This tree structure forms the \emph{scene hierarchy}. The elements within that hierarchy are called \emph{scene objects}. The scene hierarchy can be modified in the \emph{scene hierarchy view}, located on the left hand side of the scene editor. V-Rep provides various types of scene objects, but only those, which are important for the implementation will be explained here.

\begin{itemize}
\item \textbf{Shape} \\
Rigid objects like robot links or parts of the furniture are represented as shapes. V-Rep distinguishes between primitive shapes (cylinder, cuboid, sphere, plane and disk) and complex shapes (triangle meshes). Various shapes can be combined to groups and therefore treated as one single object. Shapes can be defined to be \emph{static} or \emph{non-static}. The position of a static object is fixed relative to its parent node within the scene hierarchy. Non-static objects underlie gravity and will fall down if they are not constrained by a joint or a rigid connection. It is also necessary to distinguish between \emph{respondable} and \emph{non-respondable} shapes. Respondable shapes are handled by the physics engine and create collision reactions when colliding with other respondable objects during simulation. Therefore their physical attributes like mass, moment of inertia and material settings need to be clearly defined. Non-responable shapes are visible in the scene but they do not create collision reactions. Shapes allow to set additional attributes that specify special properties, used by other modules of the simulator. The \emph{collidable} attribute states that the shape is considered by the collision detection module. The \emph{renderable} attribute marks a shape to be recognized by a vision sensor.

\item \textbf{Joint} \\
A joint is a flexible connection between two rigid parts of a robot. All the joints that were used within this project are \emph{revolute} joints which allow the rotation on a single-axis. Those joints are actuated by a motor. The motor settings include maximum force or torque and velocity limits. V-Rep supports joint control in three different modes:
\begin{itemize}

\item \textbf{Torque/force mode} \\
The behaviour of a joint in \emph{torque/force} mode is simulated by the physics engine, based on the specified motor limits. The joint is operated by simulated controller that allows to set a target position. The controller then actuates the joint towards the commanded target position. This is the most realistic control mode as the dynamic parameters of the joint motor and the connected bodies are taken into account.

\item \textbf{Inverse kinematics mode} \\
A joint in \emph{inverse kinematics mode} is controlled by V-Rep's IK calculation module. The configuration option \emph{hybrid operation} specifies, that the dynamic parameters of the joint are also taken into account.

\item \textbf{Dependent mode} \\
Operating a joint in \emph{dependent mode} means that its position depends on the position of another joint within the scene. This dependency is formulated as \emph{dependency equation} which calculates a target position based on the current position of the connected joint.

\end{itemize}
The configuration of the angular joint limits is defined based on a minimum position and the maximum possible opening angle. 

\item \textbf{Vision sensor} \\
A vision sensor is a simulated image capturing device. Each vision sensor captures images depending on its location within the scene and the configured opening angle and resolution. It is able to produce sequences of RGB and depth images as well.

\item \textbf{Force sensor} \\
A force sensor in V-Rep is used to rigidly connect two non-static, respondable shapes. The force sensor then measures the linear forces and and angular torques that act upon this connection. It is also possible to define a maximum force the connection is able to bear. When this maximum value is exceeded, the connection will break.

\item \textbf{Dummy} \\
Dummies are the simplest scene objects but they provide important functionality for the various modules of V-Rep. Dummies are used to explicitly define the origin of a specific reference frame, e.g. the world reference frame or the base of a robot arm. Other scene objects can then be configured to be child objects of such dummies within the scene hierarchy. Their relative placement can then be adjusted with respect to that reference frame. Two dummies can also be linked as \emph{tip-target pairs} to be used by the IK calculation module. This functionality is explained in section \ref{sec:config_ik}.

\end{itemize}

Groups of scene objects can be organized in \emph{collections} and treated as one single entity. Collections play an important role in the collision detection module. Let $\mathcal{A}$ and $\mathcal{B}$ be two distinctive collections of collidable shapes. The collision detection module allows to explicitly check collection $\mathcal{A}$ for collisions against collection $\mathcal{B}$. Collections also allow to override the collidable or renderable settings of all contained shapes. The configuration of the collision detection module is explained in section \ref{sec:config_col}. \\

Scene objects can be loosely arranged within the scene hierarchy or be part of a \emph{model}. A model is a subtree of elements within this hierarchy that logically belong together, as they form a specific object like a robot arm or a table. The root element of this model tree is called the \emph{model base}. Models can be saved in model files and therefore be reused in various different scenes. Robot models are composed from rigid bodies called \emph{links}, connected by joints that allow to actuate the flexible parts of the simulated robot component. Each link is usually represented by two different shapes. The first one forms the visual geometry and is usually a non-respondable, complex shape. The second shape forms the collision geometry which has to be a simplified approximation of the visual geometry, composed from groups of primitive shapes. That shape states the respondable part of the robot link as it is seen by the physics engine and needs therefore appropriate configuration of the dynamic parameters. That part is usually invisible but very important as it applies realistic behaviour to the model and allows it to interact with other respondable objects within the scene. Without respondable parts, the model would just move through other bodies as only respondable shapes are able to produce collision reactions. \\

This separation into visual and respondable part is necessary because V-Rep and the utilized physics engines are not able to use arbitrarily complex shapes for dynamics calculations. Utilized meshes have to fulfil at least the \emph{convex} criteria\footnote{A line between any two vertices of the mesh lies completely within the mesh boundaries}, but the V-Rep documentation recommends to use groups of primitive shapes for the collision geometry to increase the simulation performance. The V-Rep scene editor provides a special \emph{shape edit mode} that allows to edit and simplify meshes and to extract different types of primitive shapes from a selection of vertices. The shape edit mode can be entered by selecting the desired shape and then click the item \texttt{Tools -> shape edition} from the menu bar or the corresponding tool bar button.

\begin{figure}[ht]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/sdh_sheet.jpg}
	\caption{Schunk SDH-2 gripper}
	{\scriptsize Image source: \cite{schunk2010}}
	\label{fig:sdh_sheet}
\end{figure}

\section{Designing the simulation scene}

This section explains in detail the structure of the simulation scene and the contained models. The setup contains three types of robot component - the Kuka LWR4+ robot arm, the Schunk SDH-2 gripper and the Kinect camera. For each one of them, a simulation model was required in order to be able to build up the scene. V-Rep ships with realistic and fully functional model for the Kuka arm and also for the Kinect camera. But as there was no model for the Schunk SDH-2 robot hand available, we had to create it from scratch. The necessary steps of this modelling process are explained in the next section.

\subsection{Modelling the Schunk SDH-2 gripper}

Figure \ref{fig:sdh_sheet} shows the Schunk SDH-2 hand. The gripper has 3 fingers, each one containing two modular joints. The joints located closer to the wrist are called the \emph{proximal} finger joints whereas the joints, actuating the finger tips are called the \emph{distal} finger joints. The joints 1a and 1b in figure \ref{fig:sdh_sheet} can be rotated along their vertical axis though their rotation angles are codependent. That means if one finger rotates to the left, the other one is rotated to the right for the same angle, actually adding one additional degree of freedom. Those two joints are called the \emph{pivoting} joints. \\

\begin{figure}[t]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/place_joint.jpg}
	\caption{Placing a joint within the model}
	\label{fig:place_jnt}
\end{figure}

The visual part of the hand model basically consists of 4 different shapes - the gripper body, finger knuckles, finger links and finger tips. Suitable meshes were taken from the \path{schunk_description}\footnote{http://wiki.ros.org/schunk\_description} ROS package and imported into the V-Rep editor. They have then been arranged according to the technical description in \citep{schunk2010}. The next step was to connect those parts by the corresponding joints. \\

The correct placement of those joints is very important to achieve the desired transformations of the actuated parts when changing the joint angles. Therefore it was necessary to determine the correct position and orientation for each single joint within the model. Each flexible part of the gripper has cylinder shaped holes at the locations where the parts are connected by joints. Those holes also exist in the utilized meshes and they were used to extract the correct joint placement locations. This was achieved by selecting the vertices within the mesh that form the area, where a specific joint had to fit. From that selection a cylinder was extracted, using the corresponding editor functionality. The origin of that cylinder's reference frame which is located in the cylinder center stated the exact place location for the corresponding joint. After creating a new joint on that location, the extracted cylinder could be removed again. Those steps had to be repeated for all 8 gripper joints. This placement process is visualized in figure \ref{fig:place_jnt}. The left image shows the extraction of the target area. On the right image, the joint is already placed on its appropriate location.
\begin{figure}[h]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/hand_tree.jpg}
	\caption{Kinematic chain of the Schunk gripper}
	\label{fig:hand_tree}
\end{figure}\\

After placing all the joints and links within the scene, the model tree was adjusted to form the kinematic chain of the hand as can be seen in figure \ref{fig:hand_tree}. The dynamic parameters of the joints were set according to the technical description in \citep{schunk2010}. The configured values are listed in table \ref{tbl:tech_data}. As the positions of the pivoting joints are connected to each other, the configuration of the second finger's pivoting joint had to be set sightly different. To achieve this mirroring behaviour the joint is operated in the dependent mode and its dependency equation $f$ is defined as $f(x) = -x$ where $x$ is the position of the first finger's pivoting joint.
\begin{table}[h]
  \centering
  \begin{tabular}[h]{|l|c|c|c|c|} \hline
	\textbf{Joint type} & \textbf{pos. min.} & \textbf{pos. max.} & \textbf{max. effort} & \textbf{max. velocity} \\ \hline
	Pivoting joints & $0^{\circ}$ & $90^{\circ}$ & $5.0 Nm$ & $210^{\circ}/s$ \\
	Proximal joints & $-90^{\circ}$ & $90^{\circ}$ & $2.1 Nm$ & $210^{\circ}/s$ \\
	Distal joints & $-90^{\circ}$ & $90^{\circ}$ & $1.4 Nm$ & $210^{\circ}/s$ \\ \hline
  \end{tabular}
  \caption{Joint dynamic parameters}
  \label{tbl:tech_data}
\end{table} \\

The visual part of the gripper model was formed by the utilized meshes. The next step in the design process was to define the collision geometry that was used to model the underlying physics. We decided to do that by approximating the original shape by groups of primitive shapes. This was achieved by executing the following steps for each part of the model:
\begin{itemize}
\item
The shape edit mode was used to locate and select parts of the mesh that could be approximated by either a cuboid or a cylinder. The extraction was done by selecting groups of suitable vertices with the mouse while holding the \texttt{shift} button.
\item
Those parts were then extracted as primitive shape by using the corresponding editor functionality.
The extraction process is visualized in figure \ref{fig:ex_pure_shape}.
\item
Those steps had to be repeated until the most important parts of the shape were extracted that way. After that all extracted shapes were grouped into one single shape.
\item
The respondable flag was activated for the resulting shape and the dynamic parameters were adjusted accordingly. The mass values and the inertial matrix have been provided by Alex Rietzler who had already evaluated those values for his own project. The material settings were left at the default value which is the predefined \emph{highFrictionMaterial}. This material has a friction coefficient of 1.0 which is the maximum possible value but it uses no linear or angular damping.
\item
For the name of the extracted shape the same value was used as for its visual counterpart but the \path{_respondable} suffix was appended. The suffix allows to easily identify the respondable model parts within the scene hierarchy. 

\item
The extracted shapes were then removed from the active visibility layer because they are just used for dynamics calculations.
\end{itemize}

\begin{figure}
	\centering
  	\includegraphics[width=1.0\textwidth]{images/extract_pure_shapes.jpg}
	\caption{Process of approximating the original mesh with pure shapes}
	\label{fig:ex_pure_shape}
\end{figure}

The last step of the modelling process was the adjustment of the model tree. This adjustment was done by rearranging the nodes within the scene hierarchy view. Root element of the hand model is the respondable part of the gripper body which was also defined to be the dedicated model base. At this point it is very important to follow the V-Rep guidelines for designing dynamic simulations because if the hierarchy is wrong, the model will simply fall apart when starting the simulation (detailed information about that topic can be found in the corresponding chapter\footnote{http://www.coppeliarobotics.com/helpFiles/en/designingDynamicSimulations.htm} of the V-Rep dodumentation). Each non-static and respondable shape needs to be connected to its parent by a joint or a force sensor. The visual part of the link is always a child object of its corresponding respondable. That way, the kinematic chain of the gripper was formed. The resulting model tree is shown in figure \ref{fig:gripper_tree}.
\begin{figure}[ht]
	\centering
  	\includegraphics[width=0.4\textwidth]{images/gripper_tree.jpg}
	\caption{Gripper model tree}
	\label{fig:gripper_tree}
\end{figure}

\subsection{Assembling the scene}

\begin{figure}[hbt]
	\centering
  	\includegraphics[width=0.8\textwidth]{images/simulation_scene.jpg}
	\caption{Structure of the V-Rep simulation scene}
	\label{fig:sim_scene}
\end{figure}

After finishing the gripper modelling process, each necessary component was available to build up the simulation scene as can be seen in figure \ref{fig:sim_scene}. The final scene consists of a model of the robot's torso, two Kuka LWR4+ arm models with attached grippers and a model of the table in front of the robot. The origin of the world reference frame is located on the upper left side of the table, indicated by a slightly green shimmering sphere. A dummy object called \path{ref_frame_origin} was placed at that location. Each position for the simulation is defined relative to this dummy element. If it is necessary to change the origin of the world reference frame, this can simply be achieved by just adjusting the position and orientation that dummy. The positions and orientations of the torso, the table and the two arms were set relative to the world reference frame. The grippers were placed on the tip of each arm. Within the scene hierarchy they are child elements of the last node in the corresponding arm tree. The correct rotation and offset was measured on the real counterpart and then adjusted accordingly. \\

The plate and the legs of the table were modelled as group of primitive cuboids. The table shape is respondable to ensure that it will produce a collision reaction if a robot component collides with it. But as it is a static object its position will not be influenced by such a collision because it is fixed within the scene. The material setting is set to the default \emph{highFrictionMaterial}. \\

The utilized Kinect camera model was contained in the V-Rep distribution and just had to be inserted into the scene. Position and orientation of the Kinect camera with respect to the robot base was measured in the real world and adjusted accordingly in the simulator. Figure \ref{fig:sim_scene_tree} shows a part of the resulting scene tree for the final simulation scene. 

\begin{figure}[h]
	\centering
  	\includegraphics[width=0.4\textwidth]{images/scene_tree.jpg}
	\caption{Simulation scene hierarchy}
	\label{fig:sim_scene_tree}
\end{figure}

\subsection{Configuring the collision detection module}
\label{sec:config_col}

The final solution needs to be able to detect and visualize collisions of the simulated robot with itself or its environment. This section describes the utilized simulator functionality along with the necessary steps to achieve the desired behaviour. \\

V-Rep's \emph{collision detection module}\footnote{http://www.coppeliarobotics.com/helpFiles/en/collisionDetection.htm} is capable of detecting and visualizing collisions between (collections of) shapes. The shapes used for this collision detection can be any shapes within the scene as long as they are defined to be \emph{collidable}. It is important to note that the collision detection module only \textit{detects} collisions by checking for interferences between meshes. Handling \emph{collision reactions} is the responsibility of the  physics engine. Configuration of the collision detection module is done by registering one or more \emph{collision objects}. Each collision object consists of a \emph{collider} and a \emph{collidee}. The collider can be for example the robot arm and the collidee a collection of objects, the arm is not allowed to collide with. Both of them can be single shapes or collections of shapes. The big advantage in using collections is that they allow to exactly describe which shapes should be checked against which other ones. It is also possible to configure a collection to overwrite the collidable attributes of the contained shapes. That means a whole collection can be marked to be collidable even though the contained shapes are not. The collidee settings also offer the option \emph{all other collidable objects in the scene}. In that case, the collider is checked against all collidable shapes in the scene that are not part of the collider itself. Detected collisions are visualized by applying different colouring either to the collider or to the collidee. Each collision object needs to be \emph{handled}, which means the collision detection module has to be triggered to check for collisions and visualize them as defined for the specific collision object. This can happen automatically in each simulation step or it can be triggered manually by using the corresponding API function. The desired behaviour is defined by a configuration option called \emph{Explicit handling} in the collision object properties. \\

\begin{figure}[h]
	\centering
  	\includegraphics[width=0.8\textwidth]{images/collision.jpg}
	\caption{Collision detection and visualization}
	\label{fig:collision}
\end{figure}

One of the aims of the solution is to allow to check actions in the simulator before executing them on the real robot. Therefore the simulator needs to be able to detect and visualize collisions that possibly happen during those actions. Moreover it would be a convenient feature to have a proximity warning if parts of the robot come dangerously close to an obstacle during movement. To achieve that goal, we decided to create an enlarged version of the robot arm geometry which we called the \emph{collision shield} and do additional collision checking. These considerations lead to the two different types of possible collisions which we called \emph{soft collisions} and \emph{hard collisions}. Soft collisions are collisions, detected on the collision shield of the model. They just indicate a warning that the robot comes very close to an object it is not allowed to touch. \emph{Hard collisions} describe a direct hit of a robot part on another object. This would be also a collision in the real world. The final solution is able to distinguish between those two types as can be seen in figure \ref{fig:collision}. The left image shows a soft collision, the right image indicates a hard collision.\\

The first step during realization was to model the collision shield. Therefore it was necessary to create an additional shape for each single link of the robot arm that is slightly larger than the original one. The images in figure \ref{fig:shield_steps} show snapshots of this modelling process. For all links within the arm models the following steps had to be performed:

\begin{itemize}
\item
Create a copy of the shape that forms the visible part of the robot link
\item
Morph the copied object into a group of convex shapes to reduce complexity. This can be done by using the corresponding shape editor functionality (figure \ref{fig:shield_steps}a).
\item
Ungroup the resulting group of shapes and merge them into one single shape (figure \ref{fig:shield_steps}b).
\item
Grow the bounding box of the resulting shape, but only in x and y direction. The resulting mesh should be 10cm larger but keep the same height (figure \ref{fig:shield_steps}c).
\item
Adjust the outside color to make it green and nearly transparent. The collision shield should be visible but not occlude the original model (figure \ref{fig:shield_steps}d).
\item
Apply a meaningful name to the newly created shape to be be able to easily identify it within the model hierarchy. Here the name of the original shape with the \path{_col} suffix was used.
\item
Place the new shape as child element of the corresponding respondable shape within the model tree to ensure that the shield element correctly moves when actuating the arm model.
\item
Adjust the shape object properties. Define the new shape to be \emph{static} and \emph{non-respondable} because it is not intended to produce any collision reactions.
\item
Disable the \emph{collidable} flag on the shape. This behaviour will be overridden in the collection
settings later on when configuring the collision detection module.
\end{itemize}

\begin{figure}[h]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/shield_steps.jpg}
	\caption{Modelling steps for the second arm link}
	\label{fig:shield_steps}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}[h]{|l|l|l|} \hline
	\textbf{Collision object} & \textbf{Collider} & \textbf{Collidee} \\ \hline
	left\_arm & leftArm & all other entities \\
	right\_arm & rightArm & all other entities \\
	left\_armShield & leftArmShield & exLeftArmShield \\
	right\_armShield & rightArmShield & exRightArmShield \\ \hline
  \end{tabular}
  \caption{Configured collision objects}
  \label{fig:col_groups}
\end{table}

\begin{table}
  \centering
  \begin{tabular}[h]{|l|l|} \hline
	\textbf{Collection} & \textbf{Definition} \\ \hline
	leftArm & $\{~s~|~s\in\textit{subtree of left arm}~\}$ \\
	leftArmShield & $\{~t~|~t\in\textit{element of left collision shield}~\}$ \\
	exLeftArmShield & $\{~u~|~u\in\textit{scene and}~u~\notin\textit{subtree of left arm}~\}$ \\
	rightArm & $\{~v~|~v\in\textit{subtree of left arm}~\}$ \\
	rightArmShield & $\{~w~|~w\in\textit{element of left collision shield}~\}$ \\
	exRightArmShield & $\{~x~|~x\in\textit{scene and}~x~\notin\textit{subtree of right arm}~\}$ \\ \hline
  \end{tabular}
  \caption{Collection definitions}
  \label{fig:col_defs}
\end{table}

The next step was to define the required collision objects in the configuration of the collision detection module. The collision objects were created as listed in table \ref{fig:col_groups}. The collection definitions that were used for the collider and collidee settings can be seen in table \ref{fig:col_defs}.
Each arm requires two collision objects - one for hard collisions and another one for soft collisions. The responsibility of the collision objects named \path{left_arm} and \path{right_arm} is to check for hard collisions. The colliders (collections named \path{leftArm} and \path{rightArm}) are checked for collisions against \emph{all other collidable objects in the scene}. This setting is only possible because the collidable flag is disabled within the scene object settings of the collision shield elements as mentioned above. That means that they are excluded from collision checking by default. The collision objects, named \path{left_armShield} and \path{right_armShield} are responsible to check for soft collisions. The colliders (collections \path{leftArmShield} and \path{rightArmShield}) contain only the collision shield elements of the corresponding arm models. As those elements were defined without the \emph{collidable} flag, the option \emph{Collection overrides collidable properties} was selected in the collection settings to explicitly enforce collision checking when using those collections. The collections, defining the collidees (\path{exLeftArmShield} and \path{exRightArmShield}) include all other objects in the scene \emph{except} those, contained in the left/right arm's subtree. This exclusion is necessary because otherwise those collision objects would also detect collisions between the shield elements and the other arm links.\\

All collision objects are defined not to be handled explicitly. It is necessary to first check for direct hits before looking for shield hits because a direct hit always implies a shield hit. Therefore the collision objects have to be handled sequentially and for soft collisions is only checked if no hard collision was detected before. Handling those collision objects happens from code and will be explained later on in the section about control interface implementation.

\subsection{Configuring the IK calculation module}
\label{sec:config_ik}

The ROS control interface requires the robot arm to be controllable in joint space and in Cartesian space, depending on the selected control mode. Joint space targets are relatively easy to handle as that only means to set the target position of each single arm joint. Setting targets in Cartesian space requires to solve the \emph{inverse kinematics} (IK) problem which is defined by \citep{craig2005} as the problem of finding a possible joint configuration for the robot to achieve a desired end effector position and orientation. This section describes the IK functionality provided by the V-Rep simulator along with necessary configuration steps.\\

The \emph{inverse kinematics calculation module}\footnote{http://www.coppeliarobotics.com/helpFiles/en/inverseKinematicsModule.htm} allows to define and register \emph{IK groups}. An IK group is capable of solving IK problems for a simulated manipulator like our robot arm. It consists of one or more so called \emph{IK elements} and configuration settings for the utilized IK calculation method. IK elements are used to define the kinematic chain, constraints and desired precision settings. The kinematic chain is specified by configuring the dedicated \emph{base} link and the \emph{tip}. The tip is a dummy object, indicating the end effector reference frame. It needs to be linked to a \emph{target} dummy, forming a \emph{IK, tip-target} connection which is done by selecting the appropriate link type within the dummy object settings. The IK goal pose is set by placing that target dummy at the desired location. IK elements can be configured to enforce position constraints and/or orientation constraints for each single axis. The precision settings define the maximum allowed deviation (linear and angular) between desired and achieved end effector position to consider a solution to be correct. \\

\begin{figure}[htb]
	\centering
  	\includegraphics[width=0.5\textwidth]{images/ik_vrep.jpg}
	\caption{IK calculation module concept}
	{\scriptsize Image source: http://www.coppeliarobotics.com/helpFiles/en/solvingIkAndFk.htm}
	\label{fig:ik_vrep}
\end{figure}

The IK calculation module allows to define various IK groups for each robot component with differing configurations. Along with the IK elements, the IK group settings include the desired calculation method and the maximum amount of calculation iterations to use. Available IK calculation methods are \emph{pseudo inverse} (PI) and \emph{damped least squares} (DLS). As stated by \cite{buss2004}, the PI method is usually faster than DLS. The tradeoff is that PI tends to be unstable in configurations where the target position is unreachable. That fact leads to a jittery behaviour of the manipulator. The DLS method provides higher stability in such situations but requires more calculation time and is therefore slower. \\

A target pose in Cartesian space is set by adjusting the position and orientation of the IK target dummy. Then it is necessary to \emph{handle} one more of the responsible IK groups which follows the same approach that is used for the collision objects described above. The IK calculation module tries to adjust the joint configurations of the corresponding manipulator until the tip pose matches the target pose, respecting joint limits and configured constraints. Figure \ref{fig:ik_vrep} shows a schematic description of that concept. The joints within the kinematic chain need to be operated in \emph{inverse kinematics mode}, otherwise they can not be controlled by the IK calculation module. This can be specified within the joint settings. \\

\begin{table}[h]
  \centering
  \begin{tabular}[h]{|l|c|c|c|} \hline
	\textbf{IK group} & \textbf{Method} & \textbf{Iterations} & \textbf{Prec. lin/ang} \\ \hline
	left\_arm & PI & 9 & 0.001 / 0.1  \\
	left\_arm1 & PI & 3 & 0.002 / 0.2  \\
	left\_arm2 & DLS & 3 & 0.002 / 0.1  \\
	right\_arm & PI & 9 & 0.001 / 0.1  \\
	right\_arm1 & PI & 3 & 0.002 / 0.2  \\
	right\_arm2 & DLS & 3 & 0.002 / 0.1  \\ \hline
  \end{tabular}
  \caption{IK group definitions}
  \label{fig:ik_defs}
\end{table}

Three IK groups have been created for each arm contained in our simulation scene. The configuration settings are listed in figure \ref{fig:ik_defs}. They were chosen, following the guidelines in the V-Rep documentation. The precision settings specify the tolerance values that have to be enforced by the corresponding IK group. The first two groups use the faster PI calculation method. The first one allows a higher amount of maximum iterations while demanding stricter precision settings than the second one. The third one is designed to increase stability especially for positions close to singularities. Therefore the DLS method was chosen with an increased position tolerance value. That configuration is slower because of the DSL calculation method and therefore it is only used if the other groups failed to find a solution. The IK groups are handled sequentially until one of them is able to solve the problem. This process is explained in the ROS control interface section later on.

\section{Implementing the ROS control interface}

\begin{figure}[htb]
	\centering
  	\includegraphics[width=0.8\textwidth]{images/control_flow.jpg}
	\caption{Control flow structure}
	\label{fig:control_flow}
\end{figure}

In the real world, the Schunk hand and the Kuka arm have their own, clearly defined ROS control interfaces. Those interfaces are composed from sets of inbound and outbound ROS topics that allow to send commands and to retrieve state data. The ROS topics are usually provided by a controller that writes commanded values to the hardware and reads the current state. The structure of this control flow is visualized in figure \ref{fig:control_flow}. The simulation models need to provide exactly the same ROS interface as their real counterparts, using the same topic names and message types. The existing ROS interface that is part of the V-Rep distribution is not suitable as it only provides a very general approach for controlling joint values and reading state data so it was necessary to create a custom solution that provides proper interfaces for the arm and hand models. \\

The V-Rep simulator provides various extension points, allowing to add custom functionality. We implemented the final solution as a simulator plugin, written in C++ and using V-Rep's \emph{regular API}\footnote{http://www.coppeliarobotics.com/helpFiles/en/apiOverview.htm}. A plugin is a software library that gets automatically loaded on V-Rep startup and interacts with the simulator via the provided API functions. The plugin architecture is described in section \ref{sec:plugin_arch}. As the robot setup in the IIS-Lab is frequently extended we put particular focus on the extendability of the plugin design. During project implementation, the IIS group acquired a new robot head that is also controllable via ROS. The provided solution can easily be extended to be able to handle simulation models for such new robot components. \\

One problem that arose during the implementation process was, how to identify simulation models that should be controlled by the plugin. V-Rep is a simulation environment that allows to load and simulate different environments and therefore the plugin functionality should not be tied to one specific scene. The current simulation scene contains the models of two Kuka arms and two Schunk grippers but the intended solution should be customizable. Maybe there should be another scene that only contains one arm model and no grippers at all. This requires the plugin to be able to detect known models in arbitrary simulation scenes and then provide the proper control interfaces. Section \ref{sec:comp_id} explains in detail how this problem was solved along with the utilized V-Rep functionality. \\

\subsection{Plugin architecture}
\label{sec:plugin_arch}

This section describes the architecture of the implemented simulator plugin along with the most important parts of the software. The source code can be found in the \path{iis_simulation} package that is part of the \path{iis_robot_sw} repository.\\

A plugin is a compiled library file that needs to be placed in the V-Rep working directory and follow the V-Rep specific naming conventions. On startup, V-Rep looks for library files, prefixed with \path{libv_repExt}. Matching files are automatically loaded. A plugin runs in the main simulation thread - that means it has to be programmed really carefully to avoid performance leaks during simulation. The plugin code is organized as can be seen in the UML diagram in figure \ref{fig:plugin_uml}. The major parts of the system are described in the subsequent paragraphs.

\begin{figure}
	\centering
  	\includegraphics[width=0.8\textwidth]{images/SimulatorPluginUML.jpg}
	\caption{Simulator plugin architecture}
	\label{fig:plugin_uml}
\end{figure}

\paragraph{SimulationComponent}

We us the term \emph{simulation component} to describe a simulation model for a specific robot component that has to be handled by the plugin. Each simulation component provides its own clearly defined control interface. A simulation scene can contain multiple simulation components from various types. The \path{SimulationComponent} class is the abstract base class for all those simulation components. Currently there are existing two concrete implementations -- the \path{LWRArmComponent} and the \path{SchunkHandComponent}. If the scenario should be extended and new components have to be introduced, it is necessary to create a new subclass of \path{SimulationComponent} and provide implementations for the abstract methods. The methods \path{startOfSimulation()} and \path{endOfSimulation()} are called when a simulation is started/ended. The method \path{handleSimulation()} is invoked on each simulation step and can be used for example to handle IK groups or registered collision objects.

\paragraph{ComponentContainer}

This class represents the set of all identified simulation components within the current scene. On V-Rep startup an instance of \path{ComponentContainer} is created. When the plugin recognizes that the content of the current simulation scene has changed, the method \path{actualizeForSceneContent} of the \path{ComponentContainer} gets triggered. This method then performs the following steps:
\begin{itemize}

\item
It validates all currently registered \path{SimulationComponent} instances if they are still valid and present in the scene.
\item
It traverses the whole scene hierarchy to identify newly created components
\item
If a new component is identified, a corresponding concrete instance of \path{SimulationComponent} is created and added to the container.

\end{itemize}
  
During a running simulation, the \path{ComponentContainer} gets notified about each single simulation step. It then simply propagates that event to all registered \path{SimulationComponent} instances by invoking their \path{handleSimulation} methods. Those can then perform all necessary steps like triggering collision checking or handling IK groups.

\paragraph{ComponentController}

This is the abstract base class for all controllers. A controller actually represents the ROS interface of a specific simulation component, maintaining all the inbound and outbound topics used to control the simulated hardware. It is responsible for delegating commanded values to the underlying component as well as reading and publishing state data. Concrete implementations are the \path{LWRArmController} and the \path{SchunkHandController}. A \path{ComponentController} needs to be registered at the \path{ROSServer} and gets initialized on simulation start. Concrete implementations can use the provided \path{NodeHandle} to create all the necessary publishers and subscribers. The \path{update} method is called by the \path{ROSServer} on each simulation step and forces the controller to publish all the required data. The \path{shutdown} method is called by the \path{ROSServer} on simulation end, forcing the controller to shutdown all publishers and subscribers.
  
\paragraph{ROSServer}

The \path{ROSServer} is a static class that encapsulates all ROS related functionality. It tries to initialize ROS on plugin startup, forcing a shutdown, if the connection to the master cannot be established. Otherwise it creates and maintains a ROS \path{NodeHandle} for the \path{simulation} namespace. Each \path{SimulationComponent} can register \path{ComponentController} instances at the \path{ROSServer}. On simulation start it initializes all registered controllers with the maintained \path{NodeHandle}. The \path{ROSServer} gets also notified about each simulation step and forces the controllers to handle the received commands and publish all the necessary data. On simulation end it triggers the shutdown of all registered controllers.

\paragraph{SimulatorPlugin}

The \path{SimulatorPlugin} states the connection between the simulator and the other parts of the software. V-Rep communicates with a plugin, using a clearly defined interface that consists of 3 function definitions:

\begin{itemize}

\item \texttt{unsigned char v\_repStart(void* reserved,int reservedInt)} \\
This function is called on V-Rep startup and is used to perform necessary initialization steps. A return value of zero indicates that the initialization process failed and the plugin gets unloaded immediately.
Our implementation calls the \path{initialize()} method of the \path{ROSServer} class. If that initialization fails, the plugin is not able to work.

\item \texttt{void v\_repEnd()} \\
This function is called before V-Rep shutdown. Our solution performs necessary cleanup steps and frees all allocated memory.

\item \texttt{void* v\_repMessage(int msg, int* auxData, void* custData, int* resData)} \\
This function is called very often during the whole V-Rep lifecycle and is therefore very performance critical. Via this function the plugin gets notified about simulator events like start/end of simulation, simulation step, scene content change, scene switch and more. The event type is specified by the provided \path{msg} parameter. Our \path{SimulatorPlugin} then just passes those events to the software components that are responsible to handle the specific action by invoking the corresponding methods. For example simulation related events like start/end of simulation and simulation step are propagated to the \path{ROSServer} and the \path{ComponentContainer}. Table \ref{tbl:sim_msg} lists the messages that are handled by our implementation.

\begin{table}[ht]
  \centering
  \begin{tabular}{|l|l|} \hline
	\textbf{Message} & \textbf{Description} \\ \hline
	\path{sim_message_eventcallback_instancepass} & Indicates a scene content change \\
	\path{sim_message_eventcallback_moduleopen} & The simulation is about to start \\
	\path{sim_message_eventcallback_modulehandle} & Indicates a simulation step \\
	\path{sim_message_eventcallback_moduleclose} & End of simulation  \\ \hline
  \end{tabular}
  \caption{Simulator messages}
  \label{tbl:sim_msg}
\end{table}

\end{itemize}

Each simulation step is handled by the \path{SimulatorPlugin} as shown in figure \ref{fig:handle_sim}. The plugin receives the \path{sim_message_eventcallback_modulehandle} message from the simulator. It first asks the \path{ComponentContainer} to handle the simulation step which propagates that event to all registered \path{SimulationComponent} instances. After that, the \path{SimulatorPlugin} invokes the \path{handleSimulation} method of the \path{ROSServer}. The server then handles ROS related callbacks and forces the registered controllers to publish their state data.

\begin{figure}[ht]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/handle_sim.jpg}
	\caption{Handling a simulation step}
	\label{fig:handle_sim}
\end{figure}

\subsection{Identifying simulation components}
\label{sec:comp_id}

\begin{figure}[h]
	\centering
  	\includegraphics[width=0.8\textwidth]{images/scene_hierarchy.jpg}
	\caption{Sample scene hierarchy}
	\label{fig:scene_tree}
\end{figure}

The plugin functionality should not be tied to a specific simulation scene but to specific ``known'' models of robot components. For example adding a Kuka arm model to an arbitrary scene should force the plugin to create a corresponding controller that provides the proper ROS control topics. As visualized in figure \ref{fig:scene_tree}, each simulation scene is a hierarchy of different types of scene objects like joints, shapes, dummies or sensors. Those elements can be loose objects like a shape that was just added to the scene for a grasping simulation, but they could also be part of a simulation component like the Kuka arm or the Schunk gripper. Therefore the plugin needs to be able to identify subtrees of scene objects within this hierarchy that belong to such known models. Moreover, once a specific simulation component is identified, the plugin has to discover each part of the model tree as V-Rep does not provide a function that allows to determine all scene objects that belong to a specific model. Depending on the simulation component this can be the joints, force sensors or reference frame dummies. The scene objects in V-Rep are represented by their dedicated \emph{object handles}, which are simple integer values that uniquely identify specific objects within a scene. Interacting with scene objects using the V-Rep API requires those object handles to be known. For example setting the target position for a specific joint in the arm model requires to know the object handle that is associated to that joint. The goal of the identification process is to find the object handles of all scene objects that belong to the model. \\

\begin{figure}[h]
	\centering
  	\includegraphics[width=0.8\textwidth]{images/custom_dev_data.jpg}
	\caption{Custom developer data segments on scene object}
	{\scriptsize Image source: http://www.coppeliarobotics.com/helpFiles/en/pluginTutorial.htm}
	\label{fig:cust_dev_data}
\end{figure}

We solved this identification problem by using so called \emph{custom developer data tags}. V-Rep allows to put auxiliary data segments to each single object in the scene. The data tags are configured in the scene object properties of the corresponding scene objects. This process is described in the documentation section in appendix \ref{app:sim_doc}. Each data segment starts with a header number which is used to uniquely identify the tag data from a specific developer. We used the number `2497' for the data segments used in this project. A visualization of this concept can be seen in figure \ref{fig:cust_dev_data}. The format of those data segments can freely be chosen by the developer. We decided to use string representations of key/value pairs, separated by a colon (:). The key is an integer value that specifies the type of information the tag contains. The value segment can be used to provide additional information, e.g. the name of the arm. The left arm's model base for example is tagged with the data segment 
\begin{center}
\texttt{2497,1:left\_arm}
\end{center}
The number 2497 is the header that identifies the data segment to belong to this plugin. The data segment identifies that scene object to be the model base of a Kuka arm model $(Key=1)$ with the name \path{left_arm}. Other tags are used to identify joints, force sensors and dummies that belong the the arm or gripper models. The available keys are explained in the sections that correspond to the specific simulation components. We have placed such developer data tags on all scene objects that are relevant to the plugin. \\

The plugin code was designed to react for scene content change. This happens when objects are added or removed from the current scene or when another scene gets loaded. When actualizing for scene content change, the \texttt{ComponentContainer} traverses the scene hierarchy and looks for objects, that are tagged as known components. On success, it creates the specific instance and adds it to the container. During initialization, the concrete \texttt{SimulationComponent} implementation then traverses the rest of the model subtree to extract all the remaining parts that belong to that specific model (joints, dummies, force sensors\ldots), also by looking for tagged objects. The implementations for arm and hand model provide feedback output on the console window about the status of this initialization process and provide meaningful error messages in case that not all necessary parts of a model could be successfully located.


\subsection{The LWRArmComponent}

The \path{LWRArmComponent} is a concrete subclass of \path{SimulationComponent} and states the abstraction of a simulated Kuka LWR4+ arm model. It is composed of two parts - an instance of the \path{LWRArm} class that provides access to the whole functionality of the arm model and a corresponding \path{LWRArmController} that is responsible for the ROS interface. The \path{LWRArmComponent} registers its controller at the \path{ROSServer} on simulation start. Both classes are explained in the following paragraphs.

\paragraph{LWRArm}

This class states the abstraction of the simulated hardware of the arm, providing methods to set commanded values and access all available state data. On creation, it is passed the object handle of the model base element as constructor argument. The base is identified by the \path{LWR_ARM_COMPONENT} tag. The value segment of this tag states the name of the arm which has to be unique within the scene. During initialization it traverses the model tree and extracts all required parts by searching for tagged scene objects as described in the previous section. The parts to identify are the 7 arm joints, the force sensor on the last link of the arm, IK tip and target dummies. The corresponding tags are listed in table \ref{fig:lwr_tags}. \\

\begin{table}[ht]
  \centering
  \begin{tabularx}{\textwidth}{|l|l|l|X|} \hline
	\textbf{Constant} & \textbf{Key} & \textbf{Value} & \textbf{Description} \\ \hline
	LWR\_ARM\_COMPONENT & 1 & Arm name & Identifies Kuka LWR arm model \\
	LWR\_ARM\_JOINT & 12 & Joint name & Joint in Kuka LWR arm \\
	LWR\_ARM\_CONNECTOR & 13 & - & Force sensor on arm tip \\
	LWR\_ARM\_TIP & 14 & - & IK tip dummy \\
	LWR\_ARM\_TARGET & 15 & - & IK target dummy  \\ \hline
  \end{tabularx}
  \caption{Tag data items for LWRArmComponent}
  \label{fig:lwr_tags}
\end{table}

To fulfil the requirements for the ROS control interface, the arm needs to be able to operate either in joint control mode (FK mode) or in inverse kinematics mode (IK mode). During FK mode the joints are operated in \emph{torque/force} mode and accept target positions to be set. Therefore the \path{LWRArm} class provides a method called \path{setJointTargetPositions}. This method expects a vector, containing the desired joint target positions and delegates the commanded values to the corresponding arm joints. \\

During IK mode the joints have to be operated in \emph{inverse kinematics} mode, which means that they are controlled by V-Rep's IK calculation module. Therefore we implemented the method \path{setKinematicsMode} in the \path{LWRArm} class that uses the appropriate V-Rep API function (\path{simSetJointMode}) to switch all joints to the required mode. Setting a target pose in Cartesian space is done by placing the IK target dummy at the required location and orientation. This can be done by using the method \path{setTargetPose} of the \path{LWRArm} class. When the arm is operated in IK mode, the method \path{handleIK} is invoked on each simulation step. This method uses the API function \path{simHandleIKGroup} to handle the previously configured IK groups sequentially, until one of them is able to solve the problem and set the proper joint target positions. If all of the fail, the arm will be at least in a configuration that states an approximate solution to the problem. \\

The IK group object handles are determined during initialization based on their names (API function \path{simGetIkGroupHandle}). One IK group which uses the same name as the arm model is mandatory, leading to a configuration error message if no such group can be found. Subsequent groups are optional and must have the same name with consecutive numbering. For example an \path{LWRArm} class with name \path{right_arm} searches for IK groups named \path{right_arm1}, \path{right_arm2}\ldots. We chose that naming strategy because it allows to reconfigure the IK calculation module and introduce additional IK groups without touching the plugin code. \\

The \path{LWRArm} class also provides a method that returns the current collision state of the underlying arm model. Therefore it requires the handles of the configured collision objects which are also searched during initialization based on the arm name (API function \path{simGetCollisionHandle}). The collision objects, responsible for soft collisions and hard collisions are required. An \path{LWRArm} class with name \path{right_arm} for example searches for collision objects named \path{right_arm} and \path{right_armShield}. If one or both of them cannot be detected, an error message is stated on the console and the collision detection functionality will not work as expected. The collision objects are handled by the \path{LWRArm} class on each simulation step (API function \path{simHandleCollision}). The collision object that is responsible for detecting direct collisions is handled first. If that one detects a collision, a direct hit is reported and it is not necessary to handle the second object at all, because a direct hit always implies a shield hit as well. Only if no direct hit was detected, the second collision object is handled. The outcome can be queried, using the method \path{getCollisionState} of the \path{LWRArm} class. The possible return values are listed in table \ref{tbl:col_states}. \\

\begin{table}[h]
  \centering
  \begin{tabular}{|c|l|} \hline
	\textbf{State} & \textbf{Description} \\ \hline
	0 & no collision at all \\
	1 & hard collision \\
	2 & soft collision \\ \hline
  \end{tabular}
  \caption{Possible collision states}
  \label{tbl:col_states}
\end{table}

\paragraph{LWRArmController}

The \path{LWRArmController} is a subclass of \path{ComponentController} that encapsulates the whole ROS interface for the \path{LWRArmComponnt}. On creation it is passed a reference to the underlying \path{LWRArm}. The controller listens on different ROS topics that allow to set target positions in joint space or Cartesian space, depending on the current \emph{control mode}. We implemented 4 basic control modes - \emph{joint control}, \emph{Cartesian control}, \emph{stop} and \emph{follow} mode. Joint control, follow and stop mode require the underlying \path{LWRArm} to be operated in FK mode. For Cartesian control mode, the arm needs to be switched to IK mode. The available control modes are explained as follows:

\begin{itemize}

\item \textbf{Joint control mode} \\
The \path{LWRArmController} accepts target positions in joint space via the \path{joint_control/move} topic and the underlying \path{LWRArm} is operated in FK mode. The message basically consists of a vector, containing the desired target angles for for all arm joints, measured in \emph{radiant}. Target positions received by the controller are validated to not exceed a specified \emph{velocity limit}. The aim of this velocity limit is to prevent the arm from doing rapid motions. This is achieved by enforcing the constraint
\begin{equation}
  |c_{i}-t_{i}|<\delta, \forall ~i~\in~[0,6]
\end{equation}
where $c_{i}$ are current and $t_{i}$ are received target positions for the joints 0 to 6 and $\delta$ is the current velocity limit. If the velocity limit is violated, the message is dropped and a corresponding error message is published to the \path{sensoring/error} topic and written to the console output. The default limit is set to a value of 0.1 and can be adjusted via the \path{joint_control/set_velocity_limit} topic. Valid target positions are simply passed to the \path{setJointTargetPositions} method of the underlying \path{LWRArm} instance.

\item \textbf{Cartesian control mode} \\
The controller accepts target positions in Cartesian space via the \path{cartesian_control/move} topic. Therefore, the \path{LWRArm} needs to be operated in IK mode. There is also a Cartesian velocity limit enforced for the received target pose. The constraint is defined as
\begin{equation}
  |c_{x}-t_{x}|<\sigma ~and~ |c_{y}-t_{y}|<\sigma ~and~ |c_{z}-t_{z}|<\sigma
\end{equation}
where $c$ is the current and $t$ is the received target position in Cartesian space and $\sigma$ is the current Cartesian velocity limit. The velocity limit can be adjusted, using the \path{cartesian_control/set_velocity_limit} topic. Invalid messages are dropped and result in an error message as well, published in \path{sensoring/error} topic. Valid target poses are passed to the underlying \path{LWRArm}, by using the method \path{setTargetPose}.

\item \textbf{Stop mode} \\
The arm does not accept any movement commands at all. Ongoing motions will be stopped immediately when switching the arm into this control mode. This is done by switching the underlying \path{LWRArm} to FK mode and then setting the target positions of all joints to their current position.

\item \textbf{Follow mode} \\
We designed the follow mode to mirror the behaviour to the real robot arms. Therefore this mode is only in the  simulation solution available. The controller reads the joint states of its real counterpart from the corresponding ROS topic and uses its current position as target. Therefore, the \path{LWRArm} is operated in FK mode. This results in an exact copy of the real robot's motions. We used this mode to test the accuracy of the simulation model by carefully moving the robot arm close to positions where it collides with the table and check when collisions were detected in the simulation. 

\end{itemize}

This section only covered the implemented control modes and the corresponding ROS topics but the controller also offers a number of topics that allow to retrieve state data like current joint positions, Cartesian position, the collision state and more. A complete interface description, containing all available ROS topics and the corresponding message types can be found in the documentation, located in appendix \ref{app:sim_doc}.

\subsection{The SchunkHandComponent}

The design of the \path{SchunkHandComponent} follows the same approach that we used for the \path{LWRArmComponent}. Therefore only the most important facts will be stated here. The two composing parts are the \path{SchunkHand} and the \path{SchunkHandController}.

\begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{|l|l|l|X|} \hline
	\textbf{Constant} & \textbf{Key} & \textbf{Value} & \textbf{Description} \\ \hline
	SCHUNK\_HAND\_COMPONENT & 2 & Hand name & Identifies Schunk hand model \\
	SCHUNK\_HAND\_JOINT & 22 & Joint name & Joint in Schunk hand model \\ \hline
  \end{tabularx}
  \caption{Tag data items for SchunkHandComponent}
  \label{fig:schunk_tags}
\end{table}

\paragraph{SchunkHand}

The \path{SchunkHand} class states the abstraction of a simulated Schunk hand. The parts that have to be discovered during initialization are the 7 gripper joints and their corresponding names. Available tags are listed in table \ref{fig:schunk_tags}. The \path{SchunkHand} class provides methods to set joint positions (\path{setJointTargetPositions}) and to retrieve current joint states (\path{getJointStates}). Additionally it allows to modify the motor strength (\path{setJointMotorStrength}) for each single joint, based on a percentage of maximum force. This reflects the possibility of adjusting the motor currents in the real hand, which also leads to the effect that the motor strengths of the finger joints are increased or decreased.

\begin{figure}[h]
	\centering
  	\includegraphics[width=1.0\textwidth]{images/grasp_types.jpg}
	\caption{Grasp types}
	{\scriptsize Image source: \cite{schunk2010}}
	\label{fig:grasp_types}
\end{figure}

\paragraph{SchunkHandController}

The \path{SchunkHandController} provides an implementation of the ROS interface for the Schunk gripper model. We designed this class to reflect the interface provided by the controller for the real hand. The basic topics are quite similar to those of the \path{LWRArmController}, as there are \path{joint_control/move} for setting joint target positions or \path{joint_control/get_state} for retrieving joint states. The controller allows to set the hand posture by setting target positions for all hand joints or based on different \emph{grasps} (figure \ref{fig:grasp_types}). \\

A grasp is defined by the parameters \emph{grasp type} and \emph{close ratio}. Allowed grasp types are \emph{cylindrical}, \emph{spherical}, \emph{centrical} and \emph{parallel}. The desired close ratio is defined as a value between 0 and 1. The controller calculates the corresponding joint positions, based on those parameters and sends the desired target positions to the underlying \path{SchunkHand}. Each grasp can be expressed, using three functions -- one for the position of the pivoting joints and the other two for the positions of the distal and proximal finger joints. The utilized functions were taken from the original controller code for the real hand. The definitions are listed in table \ref{fig:grasp_defs}. The first column holds the name of the grasp type. The second column shows how the positions of the pivoting joints are calculated. It can be seen that they are not influenced by the close ratio $x$ in all 4 grasp types. The third and fourth columns show the equations that are used to calculate the target values of the proximal and distal finger joints based on close ratio $x$. \\

The grasp strength can be adjusted, using the \path{settings/set_motor_current} topic. The message basically consists of a vector, holding a value for each joint that defines a percentage of the maximum motor strength. The real gripper also provides a topic that allows to read the current motor temperatures. For the sake of consistency, the \path{SchunkHandController} also provides this topic, but as V-Rep is not able to simulate motor heatings, only constant fake values are published. A complete interface description can be found in appendix \ref{app:sim_doc}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|c|c|c|} \hline
	\textbf{Name} & \textbf{pivoting} & \textbf{proximal} & \textbf{distal} \\ \hline
	CYLINDRICAL & $0$ & $(-30+30x)\frac{\pi}{180}$ & $(30 + 35x)\frac{\pi}{180}$ \\
	PARALLEL & $0$ & $(-75+82x)\frac{\pi}{180}$ & $(75-82x)\frac{\pi}{180}$ \\
	CENTRICAL & $\frac{\pi}{3}$ & $(-75+82x)\frac{\pi}{180}$ & $(75-82x)\frac{\pi}{180}$ \\
	SPHERICAL & $\frac{\pi}{3}$ & $(-40+25x)\frac{\pi}{180}$ & $(40+15x)\frac{\pi}{180}$ \\ \hline
  \end{tabular}
  \caption{Joint position functions based on close ratio $x$}
  \label{fig:grasp_defs}
\end{table}

\subsection{Publishing Kinect camera data}

The Kinect camera model does not contain any flexible parts that have to be controlled by the plugin. But it was necessary to make the images, captured by the simulated vision sensor available via ROS topics. This was achieved by using the corresponding functionality of the V-Rep default ROS interface that allows publishing vision sensor data, using the correct message types. Therefore we created a LUA script that is associated with the base element of the camera model within the simulation scene. This script simply extracts the object ID of the vision sensor and enables publishers for the captured RGB and depth images. The corresponding topics are \path{kinect1/sensoring/rgb_image} and \path{kinect1/sensoring/depth_image}. The script code can be seen in code listing \ref{lst:kinect}. Topic names can be changed within that script and the publishers can be dissabled by simply commenting the corresponding lines of code. The script also assigns the sensor data to an additional view within the simulation scene. This is helpful because it visualizes, the camera images during simulation.

\lstset{style=customc}
\begin{minipage}{\linewidth}
\lstinputlisting[caption={LUA script, attached to Kinect model}, label=lst:kinect]{code/kinect_script.lua}
\end{minipage} \\